{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead68006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the base folder where all the GTFS folders are stored\n",
    "base_folder = r'C:\\Users\\rsiddiq2\\Downloads\\FBCENC-GTFS' #Provide the GTFS folder; place your sub folders in this folder. \n",
    "\n",
    "# List all subfolders (each subfolder represents an agency)\n",
    "agency_folders = os.listdir(base_folder)\n",
    "\n",
    "# Iterate over each folder, read the GTFS files, and assign the folder name as the agency_id\n",
    "all_agency_stops = []\n",
    "\n",
    "for folder_name in agency_folders:\n",
    "    # Construct the path to the folder\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "    \n",
    "    # Load GTFS files from this folder (adjust the paths to your GTFS file structure)\n",
    "    routes_path = os.path.join(folder_path, 'routes.txt')\n",
    "    stops_path = os.path.join(folder_path, 'stops.txt')\n",
    "    trips_path = os.path.join(folder_path, 'trips.txt')\n",
    "    stop_times_path = os.path.join(folder_path, 'stop_times.txt')\n",
    "    \n",
    "    try:\n",
    "        # Attempt to read the files, skipping gracefully if any are missing\n",
    "        routes_df = pd.read_csv(routes_path) if os.path.exists(routes_path) else pd.DataFrame()\n",
    "        stops_df = pd.read_csv(stops_path) if os.path.exists(stops_path) else pd.DataFrame()\n",
    "        trips_df = pd.read_csv(trips_path) if os.path.exists(trips_path) else pd.DataFrame()\n",
    "        stop_times_df = pd.read_csv(stop_times_path) if os.path.exists(stop_times_path) else pd.DataFrame()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"GTFS files not found in {folder_name}\")\n",
    "        continue\n",
    "    \n",
    "    # If agency_id or route_short_name is missing, assign the folder name as agency_id\n",
    "    if not routes_df.empty:\n",
    "        if 'agency_id' not in routes_df.columns:\n",
    "            routes_df['agency_id'] = folder_name  # Use the folder name as agency_id\n",
    "        if 'route_short_name' not in routes_df.columns:\n",
    "            routes_df['route_short_name'] = f\"{folder_name} route\"  # Placeholder route_short_name if missing\n",
    "    \n",
    "    # Handle the case where certain DataFrames might be empty\n",
    "    if routes_df.empty or trips_df.empty or stops_df.empty or stop_times_df.empty:\n",
    "        print(f\"Skipping {folder_name} due to missing critical files.\")\n",
    "        continue\n",
    "\n",
    "    # Perform the merges with available data\n",
    "    routes_trips_df = pd.merge(routes_df, trips_df, on='route_id', how='inner', suffixes=('', '_trips'))\n",
    "    trips_stop_times_df = pd.merge(routes_trips_df, stop_times_df, on='trip_id', how='inner', suffixes=('', '_stoptimes'))\n",
    "    agency_stops_df = pd.merge(trips_stop_times_df, stops_df, on='stop_id', how='inner', suffixes=('', '_stops'))\n",
    "    \n",
    "    # Drop duplicates and include route_short_name\n",
    "    agency_stops_df = agency_stops_df.drop_duplicates()\n",
    "    \n",
    "    # Include the folder name (agency) in the final dataset, handling missing columns gracefully\n",
    "    agency_stops_df = agency_stops_df[['agency_id', 'route_id', 'route_long_name', 'route_short_name', 'stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
    "    \n",
    "    # Add the folder name as the agency name if missing\n",
    "    agency_stops_df['agency_name'] = folder_name\n",
    "\n",
    "    # Step 1: Replace NaN in route_short_name with route_long_name\n",
    "    agency_stops_df['route_short_name'] = np.where(\n",
    "        agency_stops_df['route_short_name'].isna(),  # Check if route_short_name is NaN\n",
    "        agency_stops_df['route_long_name'],  # Use route_long_name if NaN\n",
    "        agency_stops_df['route_short_name']  # Otherwise, use route_short_name\n",
    "    )\n",
    "\n",
    "    # Step 2: Ensure 'agency_name' and 'route_short_name' are strings, then create 'Route_name'\n",
    "    agency_stops_df['Route_name'] = agency_stops_df['agency_name'].astype(str) + '_' + agency_stops_df['route_short_name'].astype(str)\n",
    "\n",
    "    # Append to the list of all agency stops\n",
    "    all_agency_stops.append(agency_stops_df)\n",
    "\n",
    "# Concatenate all stops from all agencies into a single DataFrame\n",
    "if all_agency_stops:\n",
    "    final_agency_stops = pd.concat(all_agency_stops)\n",
    "\n",
    "    # Drop duplicates from the final DataFrame based on relevant columns\n",
    "    final_agency_stops = final_agency_stops.drop_duplicates(subset=['agency_id', 'route_id', 'stop_id'])\n",
    "\n",
    "    # Save the final DataFrame to a CSV (commented out for now)\n",
    "    # final_agency_stops.to_csv('path/to/output_all_agency_stops.csv', index=False)\n",
    "\n",
    "    print(\"All agency stops data, including 'Route_name', saved successfully.\")\n",
    "else:\n",
    "    print(\"No valid data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_agency_stops.to_csv(r'C:\\Users', index=False) \n",
    "#save to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04616ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define the base folder where all the GTFS folders are stored\n",
    "base_folder = r'C:\\Users\\' #GTFS folder \n",
    "\n",
    "# List all subfolders (each subfolder represents an agency)\n",
    "agency_folders = os.listdir(base_folder)\n",
    "\n",
    "# Initialize a GeoJSON structure\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": []\n",
    "}\n",
    "\n",
    "# Iterate over each folder, read the GTFS files, and assign the folder name as the agency_id\n",
    "for folder_name in agency_folders:\n",
    "    # Construct the path to the folder\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "    \n",
    "    # Load GTFS files from this folder (adjust the paths to your GTFS file structure)\n",
    "    routes_path = os.path.join(folder_path, 'routes.txt')\n",
    "    shapes_path = os.path.join(folder_path, 'shapes.txt')\n",
    "    trips_path = os.path.join(folder_path, 'trips.txt')\n",
    "    \n",
    "    try:\n",
    "        routes_df = pd.read_csv(routes_path)\n",
    "        shapes_df = pd.read_csv(shapes_path)\n",
    "        trips_df = pd.read_csv(trips_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"GTFS files not found in {folder_name}\")\n",
    "        continue\n",
    "\n",
    "    # Check if trips.txt has 'route_id' and 'shape_id'\n",
    "    if 'route_id' in trips_df.columns and 'shape_id' in trips_df.columns:\n",
    "        # Merge trips.txt with routes.txt on 'route_id'\n",
    "        routes_trips_df = pd.merge(trips_df[['route_id', 'shape_id']], routes_df, on='route_id', how='left')\n",
    "\n",
    "        # Now merge with shapes.txt on 'shape_id'\n",
    "        merged_df = pd.merge(shapes_df, routes_trips_df, on='shape_id', how='left')\n",
    "\n",
    "        # Sort the merged dataframe by shape_id and shape_pt_sequence\n",
    "        merged_df = merged_df.sort_values(by=['shape_id', 'shape_pt_sequence'])\n",
    "\n",
    "        for shape_id in merged_df['shape_id'].unique():\n",
    "            # Extract the shape coordinates for this shape_id and ensure they are ordered by shape_pt_sequence\n",
    "            route_shapes = merged_df[merged_df['shape_id'] == shape_id]\n",
    "            coordinates = route_shapes[['shape_pt_lon', 'shape_pt_lat']].values.tolist()\n",
    "\n",
    "            # Convert shape_id to string\n",
    "            shape_id_str = str(shape_id)\n",
    "\n",
    "            # Handle missing route_id\n",
    "            if pd.isna(route_shapes['route_id'].values[0]):\n",
    "                route_id = \"unknown_route_id\"  # Assign a default route_id when missing\n",
    "            else:\n",
    "                route_id = str(route_shapes['route_id'].values[0])  # Treat route_id as a string\n",
    "\n",
    "            # Extract route-specific information\n",
    "            route_long_name = str(route_shapes['route_long_name'].values[0] if 'route_long_name' in route_shapes.columns else 'N/A')\n",
    "            route_short_name = str(route_shapes['route_short_name'].values[0] if 'route_short_name' in route_shapes.columns else 'N/A')\n",
    "\n",
    "            # Create a combined route name (use short name if available, otherwise long name)\n",
    "            route_name = f\"{route_short_name}\" if route_short_name != 'N/A' else f\"{route_long_name}\"\n",
    "\n",
    "            # Convert all fields to standard Python types\n",
    "            route_long_name = str(route_long_name)\n",
    "            route_short_name = str(route_short_name)\n",
    "            route_name = str(route_name)\n",
    "\n",
    "            # Ensure any int64 or float64 values are converted to Python native types\n",
    "            shape_id_str = str(route_shapes['shape_id'].values[0])\n",
    "            route_id = str(route_shapes['route_id'].values[0])\n",
    "\n",
    "            # Create a GeoJSON feature for this route\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"properties\": {\n",
    "                    \"shape_id\": shape_id_str,  # Use shape_id as a string\n",
    "                    \"agency_name\": folder_name,  # Using folder name as agency name\n",
    "                    \"route_id\": route_id,      # Add route_id as string\n",
    "                    \"route_name\": route_name,  # Combined route name\n",
    "                    \"route_long_name\": route_long_name,  # Add route long name\n",
    "                    \"route_short_name\": route_short_name,  # Add route short name\n",
    "                },\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"MultiLineString\",\n",
    "                    \"coordinates\": [coordinates]  # Wrap in a list to match MultiLineString format\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Add this feature to the GeoJSON structure\n",
    "            geojson[\"features\"].append(feature)\n",
    "\n",
    "    else:\n",
    "        print(f\"'route_id' or 'shape_id' missing in trips.txt for {folder_name}. Skipping this folder.\")\n",
    "\n",
    "# Save the final GeoJSON to a file\n",
    "output_file = 'combined_routes_with_shapes_and_names_all_agencies.geojson'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(geojson, f, indent=2)\n",
    "\n",
    "print(f\"All agency routes have been combined and saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbf001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the GeoJSON file\n",
    "geojson_file = \"C:\\\\Users\\....\\\\combined_routes_with_shapes_and_names_all_agencies.geojson\"\n",
    "gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "# Save to a shapefile\n",
    "shapefile_path = \"output_shapefile.shp\"\n",
    "gdf.to_file(shapefile_path, driver='ESRI Shapefile')\n",
    "\n",
    "print(f\"GeoJSON file has been converted to Shapefile and saved at {shapefile_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
